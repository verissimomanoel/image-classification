{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = './cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e9d82ed30>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return ( x - np.min(x) ) / ( np.max(x) - np.min(x) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "x = tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # Weight and bias\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[3], \n",
    "                                              conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.random_normal([conv_num_outputs], stddev=0.1))\n",
    "    \n",
    "    # Applying convolution\n",
    "    x_tensor = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    # Add bias\n",
    "    x_tensor = tf.nn.bias_add(x_tensor, bias)\n",
    "\n",
    "    # Apply function of activation\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "    \n",
    "    # Apply Max Pooling\n",
    "    x_tensor = tf.nn.max_pool(x_tensor, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                              strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (8, 8)\n",
    "    conv_strides = (4, 4)\n",
    "    pool_ksize = (4, 4)\n",
    "    pool_strides = (2, 2)\n",
    "    num_outputs = 10\n",
    "    \n",
    "    # 1, 2, and 3 Convolution and Max Pool layers\n",
    "    conv_maxpool_layer_1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, \n",
    "                                            pool_ksize, pool_strides)\n",
    "\n",
    "    conv_maxpool_layer_2 = conv2d_maxpool(conv_maxpool_layer_1, conv_num_outputs, conv_ksize, conv_strides, \n",
    "                                            pool_ksize, pool_strides)\n",
    "\n",
    "    conv_maxpool_layer_3 = conv2d_maxpool(conv_maxpool_layer_2, conv_num_outputs, conv_ksize, conv_strides,\n",
    "                                            pool_ksize, pool_strides)\n",
    "\n",
    "    # Flatten Layer\n",
    "    flatten_layer = flatten(conv_maxpool_layer_3)\n",
    "\n",
    "    # 1, 2, and 3 Fully Connected Layers\n",
    "    fully_conn_layer_1 = fully_conn(flatten_layer, num_outputs * 50)\n",
    "    fully_conn_layer_1 = tf.nn.dropout(fully_conn_layer_1, keep_prob)\n",
    "    fully_conn_layer_2 = fully_conn(fully_conn_layer_1, num_outputs * 10)\n",
    "    fully_conn_layer_2 = tf.nn.dropout(fully_conn_layer_2, keep_prob)\n",
    "    fully_conn_layer_3 = fully_conn(fully_conn_layer_2, num_outputs * 5)\n",
    "    fully_conn_layer_3 = tf.nn.dropout(fully_conn_layer_3, keep_prob)\n",
    "\n",
    "    # Output Layer\n",
    "    output_layer = output(fully_conn_layer_3, num_outputs)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    accur = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "\n",
    "    print('Loss: {:.4f} Accuracy: {:.2f}%'.format(loss, accur * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1024\n",
    "keep_probability = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2014 Accuracy: 18.14%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.0423 Accuracy: 23.02%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.9296 Accuracy: 27.74%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.8502 Accuracy: 28.72%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.8443 Accuracy: 30.40%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.7583 Accuracy: 35.42%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.6916 Accuracy: 35.76%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.6606 Accuracy: 37.26%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.5971 Accuracy: 38.92%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.5502 Accuracy: 41.46%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.5324 Accuracy: 41.64%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.4919 Accuracy: 42.12%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.4600 Accuracy: 42.90%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.4326 Accuracy: 43.50%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.4119 Accuracy: 43.50%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.4085 Accuracy: 42.10%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.3743 Accuracy: 44.08%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.3358 Accuracy: 44.82%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.3662 Accuracy: 45.06%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.2964 Accuracy: 45.80%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.2658 Accuracy: 45.46%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.2533 Accuracy: 46.44%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.2255 Accuracy: 46.74%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.2405 Accuracy: 46.70%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.2163 Accuracy: 46.96%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.1918 Accuracy: 47.48%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.1607 Accuracy: 47.80%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.1418 Accuracy: 48.14%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.1338 Accuracy: 48.18%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.1048 Accuracy: 48.74%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.1040 Accuracy: 48.84%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.1124 Accuracy: 48.32%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.0791 Accuracy: 47.82%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.0517 Accuracy: 49.36%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.1021 Accuracy: 48.50%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.0122 Accuracy: 50.26%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.0143 Accuracy: 50.12%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.0535 Accuracy: 49.02%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.9983 Accuracy: 50.20%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.0195 Accuracy: 49.50%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.0123 Accuracy: 49.18%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.0105 Accuracy: 49.18%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.0959 Accuracy: 47.20%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.0336 Accuracy: 48.62%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.0219 Accuracy: 48.70%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.9670 Accuracy: 50.14%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.9613 Accuracy: 49.94%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.9307 Accuracy: 50.02%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.9124 Accuracy: 49.70%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.8931 Accuracy: 50.42%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.8672 Accuracy: 50.84%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.8625 Accuracy: 50.50%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.8826 Accuracy: 50.70%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.8686 Accuracy: 50.32%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.8435 Accuracy: 51.16%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.8169 Accuracy: 51.14%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.8263 Accuracy: 50.72%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.8232 Accuracy: 50.80%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.8803 Accuracy: 50.26%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.9099 Accuracy: 48.12%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.8433 Accuracy: 50.78%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.7848 Accuracy: 51.34%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.7792 Accuracy: 51.32%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.7574 Accuracy: 51.44%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.7373 Accuracy: 51.48%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.7264 Accuracy: 51.72%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.7200 Accuracy: 51.30%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.7111 Accuracy: 51.32%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.7076 Accuracy: 51.58%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.7128 Accuracy: 51.82%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.7082 Accuracy: 51.22%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.7263 Accuracy: 50.34%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.7221 Accuracy: 49.84%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.6846 Accuracy: 51.02%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.6899 Accuracy: 51.10%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.6976 Accuracy: 51.40%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.7094 Accuracy: 50.54%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.7920 Accuracy: 49.04%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.7290 Accuracy: 49.90%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.8080 Accuracy: 47.60%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.7148 Accuracy: 50.10%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.7230 Accuracy: 50.84%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.7554 Accuracy: 50.68%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.6906 Accuracy: 51.46%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.7678 Accuracy: 49.74%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.6953 Accuracy: 51.28%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.6421 Accuracy: 50.86%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.5904 Accuracy: 51.96%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.5833 Accuracy: 51.08%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.6187 Accuracy: 50.88%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.6053 Accuracy: 51.22%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.5902 Accuracy: 51.86%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.5522 Accuracy: 51.78%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.5692 Accuracy: 51.98%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.5968 Accuracy: 51.08%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.5562 Accuracy: 52.04%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.5546 Accuracy: 51.14%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.5782 Accuracy: 50.74%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.5241 Accuracy: 51.34%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.5184 Accuracy: 52.50%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2342 Accuracy: 18.48%\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.0894 Accuracy: 24.18%\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.9272 Accuracy: 28.30%\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.9079 Accuracy: 29.88%\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.8269 Accuracy: 33.58%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.7590 Accuracy: 34.50%\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.7633 Accuracy: 37.22%\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.6397 Accuracy: 37.82%\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.6392 Accuracy: 39.12%\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.6491 Accuracy: 39.46%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.5864 Accuracy: 40.68%\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.6119 Accuracy: 41.62%\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.4821 Accuracy: 42.68%\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.5157 Accuracy: 42.90%\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.5251 Accuracy: 43.74%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.4700 Accuracy: 44.64%\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.5529 Accuracy: 43.16%\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.4373 Accuracy: 42.98%\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.4331 Accuracy: 46.08%\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.4619 Accuracy: 46.40%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.4111 Accuracy: 46.92%\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.4642 Accuracy: 46.54%\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.3623 Accuracy: 46.74%\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.3630 Accuracy: 48.00%\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.4138 Accuracy: 48.02%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.3536 Accuracy: 48.54%\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.4283 Accuracy: 46.42%\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.3250 Accuracy: 46.92%\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.3375 Accuracy: 49.56%\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.3599 Accuracy: 49.06%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.3217 Accuracy: 49.58%\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.3829 Accuracy: 48.54%\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.2886 Accuracy: 48.82%\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.2864 Accuracy: 50.40%\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.3293 Accuracy: 50.12%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.2658 Accuracy: 51.40%\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.3236 Accuracy: 50.04%\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.2520 Accuracy: 50.64%\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.2512 Accuracy: 51.62%\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.2810 Accuracy: 52.10%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.2422 Accuracy: 51.60%\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.2927 Accuracy: 50.76%\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.2169 Accuracy: 51.36%\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.2134 Accuracy: 52.12%\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.2376 Accuracy: 53.16%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.2091 Accuracy: 52.26%\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.2564 Accuracy: 51.78%\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.1774 Accuracy: 53.18%\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.1762 Accuracy: 52.38%\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.2151 Accuracy: 53.24%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.1654 Accuracy: 53.08%\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.2421 Accuracy: 52.24%\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.1642 Accuracy: 52.98%\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.1590 Accuracy: 53.34%\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.1970 Accuracy: 53.98%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.1599 Accuracy: 53.90%\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.2207 Accuracy: 52.84%\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.1537 Accuracy: 53.14%\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.1507 Accuracy: 53.86%\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.1674 Accuracy: 54.52%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.1388 Accuracy: 54.62%\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.1649 Accuracy: 55.10%\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.0993 Accuracy: 55.08%\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.1320 Accuracy: 54.24%\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.1242 Accuracy: 55.32%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.0974 Accuracy: 54.92%\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.1667 Accuracy: 53.96%\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 1.0779 Accuracy: 55.00%\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.1167 Accuracy: 54.52%\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.0960 Accuracy: 56.10%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.0870 Accuracy: 55.56%\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.1294 Accuracy: 55.80%\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.1330 Accuracy: 54.18%\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.1237 Accuracy: 53.96%\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.0973 Accuracy: 56.56%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.1042 Accuracy: 55.78%\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.1133 Accuracy: 56.06%\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.0781 Accuracy: 55.56%\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.0553 Accuracy: 56.00%\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.0666 Accuracy: 56.38%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.0972 Accuracy: 55.98%\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.1332 Accuracy: 54.82%\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 1.1077 Accuracy: 53.78%\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.0575 Accuracy: 56.56%\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.0789 Accuracy: 55.10%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.0648 Accuracy: 56.82%\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.1042 Accuracy: 56.24%\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.0807 Accuracy: 55.22%\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.0407 Accuracy: 56.98%\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 1.0594 Accuracy: 55.82%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.0486 Accuracy: 57.10%\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.0897 Accuracy: 55.46%\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 1.0890 Accuracy: 55.04%\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.0692 Accuracy: 55.90%\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 1.0244 Accuracy: 57.16%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.0882 Accuracy: 55.76%\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.0778 Accuracy: 56.36%\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.1054 Accuracy: 54.62%\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.0252 Accuracy: 58.00%\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 1.0040 Accuracy: 57.46%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.0219 Accuracy: 57.32%\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.0372 Accuracy: 57.70%\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.0602 Accuracy: 55.16%\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 1.0481 Accuracy: 56.40%\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 1.0052 Accuracy: 57.98%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.0049 Accuracy: 57.68%\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.0348 Accuracy: 58.16%\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 1.0312 Accuracy: 55.82%\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 1.0223 Accuracy: 56.60%\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.9874 Accuracy: 58.30%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.9848 Accuracy: 58.00%\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.0116 Accuracy: 57.68%\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 1.0251 Accuracy: 56.02%\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.9977 Accuracy: 57.28%\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.9836 Accuracy: 58.34%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.9715 Accuracy: 58.46%\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.9930 Accuracy: 58.02%\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.9841 Accuracy: 56.76%\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.9842 Accuracy: 57.58%\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.9532 Accuracy: 58.74%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.9735 Accuracy: 58.08%\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.9787 Accuracy: 58.62%\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.9434 Accuracy: 58.00%\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.9703 Accuracy: 57.70%\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.9385 Accuracy: 59.56%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.9441 Accuracy: 58.48%\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.9639 Accuracy: 59.02%\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.9292 Accuracy: 58.02%\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.9434 Accuracy: 58.34%\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.9192 Accuracy: 59.22%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.9471 Accuracy: 58.80%\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.9406 Accuracy: 59.36%\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.9099 Accuracy: 58.20%\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.9340 Accuracy: 58.04%\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.9037 Accuracy: 59.36%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.9068 Accuracy: 59.00%\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.9343 Accuracy: 59.14%\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.9004 Accuracy: 58.38%\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.9093 Accuracy: 58.76%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.8875 Accuracy: 59.44%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.9130 Accuracy: 59.04%\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.9083 Accuracy: 59.40%\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.8895 Accuracy: 58.54%\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.9160 Accuracy: 58.42%\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.8952 Accuracy: 59.42%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.9022 Accuracy: 58.80%\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.9144 Accuracy: 59.28%\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.8810 Accuracy: 58.70%\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.8993 Accuracy: 59.24%\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.8752 Accuracy: 59.62%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.8821 Accuracy: 59.58%\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.8867 Accuracy: 59.58%\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.8993 Accuracy: 58.30%\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.8954 Accuracy: 58.54%\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.8623 Accuracy: 59.38%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.8901 Accuracy: 59.52%\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.8759 Accuracy: 59.74%\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.8562 Accuracy: 58.88%\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.9013 Accuracy: 58.60%\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.8480 Accuracy: 59.98%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.8800 Accuracy: 59.52%\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.8730 Accuracy: 59.94%\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.8718 Accuracy: 57.90%\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.8790 Accuracy: 59.12%\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.8400 Accuracy: 59.66%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.8852 Accuracy: 59.66%\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.8718 Accuracy: 59.76%\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.8427 Accuracy: 59.02%\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.8595 Accuracy: 58.98%\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.8503 Accuracy: 59.22%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.8740 Accuracy: 59.88%\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.8963 Accuracy: 59.52%\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.8635 Accuracy: 58.72%\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.8838 Accuracy: 58.82%\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.9112 Accuracy: 57.52%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.8763 Accuracy: 59.84%\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.8861 Accuracy: 58.70%\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.8373 Accuracy: 58.78%\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.8502 Accuracy: 59.42%\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.8578 Accuracy: 59.02%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.8702 Accuracy: 59.32%\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.8635 Accuracy: 59.36%\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.8386 Accuracy: 58.26%\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.8988 Accuracy: 58.04%\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.8630 Accuracy: 58.60%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.8496 Accuracy: 59.34%\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.8711 Accuracy: 58.80%\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.7815 Accuracy: 59.36%\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.8585 Accuracy: 58.54%\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.8385 Accuracy: 58.36%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.8138 Accuracy: 59.90%\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.8406 Accuracy: 59.64%\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.7731 Accuracy: 59.74%\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.8387 Accuracy: 59.28%\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.8265 Accuracy: 59.00%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.8416 Accuracy: 59.12%\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.8336 Accuracy: 60.18%\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.7966 Accuracy: 59.28%\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.8411 Accuracy: 59.76%\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.8115 Accuracy: 59.26%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.8289 Accuracy: 59.22%\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.8254 Accuracy: 60.22%\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.7763 Accuracy: 59.74%\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.8241 Accuracy: 60.48%\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.7948 Accuracy: 59.64%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.8124 Accuracy: 59.54%\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.7945 Accuracy: 60.90%\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.7758 Accuracy: 59.58%\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.8162 Accuracy: 59.86%\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.7691 Accuracy: 60.44%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.7838 Accuracy: 59.56%\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.7778 Accuracy: 61.42%\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.7461 Accuracy: 60.76%\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.7976 Accuracy: 60.52%\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.7503 Accuracy: 60.40%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.7906 Accuracy: 59.80%\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.7692 Accuracy: 61.64%\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.7546 Accuracy: 60.08%\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.7860 Accuracy: 59.90%\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.7565 Accuracy: 60.34%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.7599 Accuracy: 60.68%\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.7716 Accuracy: 60.76%\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.7303 Accuracy: 60.82%\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.7635 Accuracy: 60.52%\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.7476 Accuracy: 59.82%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.7490 Accuracy: 61.20%\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.7643 Accuracy: 60.46%\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.7226 Accuracy: 60.64%\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.7652 Accuracy: 59.80%\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.7319 Accuracy: 60.40%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.7477 Accuracy: 61.12%\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.7490 Accuracy: 60.90%\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.7005 Accuracy: 60.46%\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.7427 Accuracy: 60.46%\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.7177 Accuracy: 60.46%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.7432 Accuracy: 60.90%\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.7453 Accuracy: 60.60%\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.6990 Accuracy: 60.56%\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.7511 Accuracy: 60.28%\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.7384 Accuracy: 59.58%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.7333 Accuracy: 61.32%\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.7300 Accuracy: 61.00%\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.6942 Accuracy: 60.64%\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.7229 Accuracy: 60.70%\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.7148 Accuracy: 60.36%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.7188 Accuracy: 61.08%\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.7292 Accuracy: 60.30%\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.7068 Accuracy: 60.28%\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.7534 Accuracy: 60.56%\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.7293 Accuracy: 59.72%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.7256 Accuracy: 60.94%\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.7244 Accuracy: 60.56%\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.6995 Accuracy: 60.70%\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.7565 Accuracy: 60.22%\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.7410 Accuracy: 59.42%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.7651 Accuracy: 60.32%\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.7652 Accuracy: 59.50%\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.7005 Accuracy: 61.32%\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.7361 Accuracy: 60.44%\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.7442 Accuracy: 59.86%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.7360 Accuracy: 61.36%\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.7530 Accuracy: 59.28%\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.7535 Accuracy: 59.66%\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.7600 Accuracy: 59.38%\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.7604 Accuracy: 59.10%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.7459 Accuracy: 61.08%\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.7755 Accuracy: 57.92%\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.7550 Accuracy: 60.06%\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.7777 Accuracy: 59.92%\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.7237 Accuracy: 60.40%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.7943 Accuracy: 58.08%\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.7413 Accuracy: 61.50%\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.6969 Accuracy: 60.08%\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.7397 Accuracy: 60.10%\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.7072 Accuracy: 60.60%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.7657 Accuracy: 58.70%\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.7231 Accuracy: 60.60%\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.7018 Accuracy: 60.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.7355 Accuracy: 59.36%\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.7764 Accuracy: 58.78%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.7270 Accuracy: 60.36%\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.7594 Accuracy: 59.80%\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.7310 Accuracy: 59.56%\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.7392 Accuracy: 60.04%\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.7216 Accuracy: 60.06%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.6991 Accuracy: 61.12%\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.7290 Accuracy: 60.10%\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.6918 Accuracy: 59.58%\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.7097 Accuracy: 60.78%\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.7146 Accuracy: 59.94%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.6931 Accuracy: 60.66%\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.6899 Accuracy: 60.72%\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.6567 Accuracy: 59.88%\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.6829 Accuracy: 61.10%\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.6694 Accuracy: 60.94%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.6790 Accuracy: 61.64%\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.7011 Accuracy: 60.36%\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.6352 Accuracy: 60.46%\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.6620 Accuracy: 61.42%\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.6674 Accuracy: 60.76%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.6655 Accuracy: 61.16%\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.6754 Accuracy: 60.08%\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.6587 Accuracy: 59.96%\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.6666 Accuracy: 60.76%\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.6649 Accuracy: 60.56%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.6458 Accuracy: 61.30%\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.6847 Accuracy: 60.02%\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.6860 Accuracy: 59.58%\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.6554 Accuracy: 61.02%\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.6676 Accuracy: 60.92%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.6641 Accuracy: 60.96%\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.6478 Accuracy: 61.08%\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.6844 Accuracy: 60.40%\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.6799 Accuracy: 60.40%\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.6721 Accuracy: 60.80%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.6538 Accuracy: 61.50%\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.6675 Accuracy: 61.24%\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.6787 Accuracy: 60.14%\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.6725 Accuracy: 60.14%\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.6809 Accuracy: 60.46%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.6827 Accuracy: 60.24%\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.6995 Accuracy: 60.98%\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.6598 Accuracy: 60.42%\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.6865 Accuracy: 60.58%\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.6628 Accuracy: 60.98%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.6577 Accuracy: 60.78%\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.6864 Accuracy: 61.06%\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.6411 Accuracy: 60.92%\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.6758 Accuracy: 61.08%\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.6350 Accuracy: 60.80%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.6324 Accuracy: 60.96%\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.7000 Accuracy: 60.70%\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.6845 Accuracy: 59.78%\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.6919 Accuracy: 60.18%\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.6494 Accuracy: 60.92%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.6467 Accuracy: 60.20%\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.7149 Accuracy: 59.76%\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.6573 Accuracy: 61.22%\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.7478 Accuracy: 60.14%\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.6755 Accuracy: 61.22%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.6800 Accuracy: 60.12%\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.7370 Accuracy: 59.66%\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.6444 Accuracy: 61.66%\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.6921 Accuracy: 60.70%\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.6571 Accuracy: 61.50%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.6640 Accuracy: 62.10%\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.6755 Accuracy: 60.86%\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.6696 Accuracy: 60.88%\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.7299 Accuracy: 59.40%\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.6456 Accuracy: 61.52%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.6650 Accuracy: 61.36%\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.6479 Accuracy: 61.44%\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.6391 Accuracy: 60.82%\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.7123 Accuracy: 60.30%\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.6555 Accuracy: 61.48%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.6759 Accuracy: 61.12%\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.6457 Accuracy: 61.44%\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.5983 Accuracy: 61.22%\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.6904 Accuracy: 60.74%\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.6400 Accuracy: 61.42%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.6500 Accuracy: 61.30%\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.6247 Accuracy: 61.40%\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.6312 Accuracy: 59.88%\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.6643 Accuracy: 60.84%\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.6142 Accuracy: 62.28%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.6036 Accuracy: 61.72%\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.6224 Accuracy: 61.68%\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.5773 Accuracy: 61.90%\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.6695 Accuracy: 60.58%\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.6145 Accuracy: 61.88%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.5959 Accuracy: 61.72%\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.6256 Accuracy: 61.36%\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.6015 Accuracy: 60.66%\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.6293 Accuracy: 60.64%\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.5893 Accuracy: 62.00%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.5930 Accuracy: 61.72%\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.6458 Accuracy: 61.22%\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.5808 Accuracy: 61.30%\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.6569 Accuracy: 59.04%\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.5705 Accuracy: 62.12%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.5838 Accuracy: 61.52%\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.6309 Accuracy: 61.20%\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.5977 Accuracy: 60.46%\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.6665 Accuracy: 59.44%\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.6276 Accuracy: 60.78%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.6100 Accuracy: 60.90%\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.6025 Accuracy: 62.04%\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.5829 Accuracy: 61.62%\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.6480 Accuracy: 59.62%\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.6081 Accuracy: 60.88%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.5996 Accuracy: 60.96%\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.5932 Accuracy: 62.26%\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.5454 Accuracy: 62.34%\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.6023 Accuracy: 60.56%\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.5755 Accuracy: 60.94%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.5799 Accuracy: 61.60%\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.5754 Accuracy: 62.44%\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.5422 Accuracy: 62.38%\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.5986 Accuracy: 61.14%\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.5334 Accuracy: 61.98%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.5581 Accuracy: 61.30%\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.5564 Accuracy: 62.06%\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.5204 Accuracy: 62.92%\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.5751 Accuracy: 61.38%\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.5085 Accuracy: 61.98%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.5561 Accuracy: 61.18%\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.5488 Accuracy: 62.30%\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.5044 Accuracy: 63.42%\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.5544 Accuracy: 61.28%\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.5015 Accuracy: 61.84%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.5502 Accuracy: 61.70%\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.5426 Accuracy: 61.92%\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.4907 Accuracy: 63.06%\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.5493 Accuracy: 61.68%\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.4959 Accuracy: 61.64%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.5645 Accuracy: 61.50%\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.5619 Accuracy: 61.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.5289 Accuracy: 61.50%\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.5834 Accuracy: 61.16%\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.4947 Accuracy: 61.78%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.5350 Accuracy: 61.88%\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.5593 Accuracy: 60.94%\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.5276 Accuracy: 61.72%\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.6019 Accuracy: 60.42%\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.5090 Accuracy: 61.66%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.5473 Accuracy: 61.72%\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.5663 Accuracy: 60.82%\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.5196 Accuracy: 61.54%\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.6618 Accuracy: 58.62%\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.5462 Accuracy: 61.10%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.5616 Accuracy: 60.50%\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.5558 Accuracy: 61.90%\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.6096 Accuracy: 60.46%\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.6361 Accuracy: 59.38%\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.5123 Accuracy: 61.64%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.5561 Accuracy: 62.00%\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.5824 Accuracy: 61.16%\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.5612 Accuracy: 61.58%\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.5809 Accuracy: 61.78%\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.5330 Accuracy: 60.94%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.5552 Accuracy: 61.36%\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.6120 Accuracy: 60.38%\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.5144 Accuracy: 62.38%\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.5780 Accuracy: 60.34%\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.5559 Accuracy: 60.80%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.5559 Accuracy: 61.58%\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.5844 Accuracy: 60.54%\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.5226 Accuracy: 62.50%\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.5800 Accuracy: 60.44%\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.5653 Accuracy: 61.02%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.5772 Accuracy: 60.72%\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.6072 Accuracy: 59.76%\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.5381 Accuracy: 61.08%\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.6016 Accuracy: 60.64%\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.5254 Accuracy: 60.98%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.5640 Accuracy: 61.20%\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.5743 Accuracy: 60.66%\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.5400 Accuracy: 61.08%\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.5592 Accuracy: 61.26%\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.5290 Accuracy: 60.98%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.5304 Accuracy: 61.40%\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.5721 Accuracy: 59.88%\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.5378 Accuracy: 60.90%\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.5809 Accuracy: 60.44%\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.5202 Accuracy: 60.40%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.5293 Accuracy: 61.74%\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.5491 Accuracy: 60.50%\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.4968 Accuracy: 61.16%\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.5834 Accuracy: 60.32%\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.4848 Accuracy: 61.00%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.5109 Accuracy: 60.88%\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.5203 Accuracy: 61.24%\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.5012 Accuracy: 61.70%\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.5626 Accuracy: 60.54%\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.4893 Accuracy: 60.76%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.5170 Accuracy: 61.18%\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.5069 Accuracy: 61.76%\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.4764 Accuracy: 62.18%\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.5436 Accuracy: 60.98%\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.4969 Accuracy: 60.86%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.5348 Accuracy: 60.68%\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.5612 Accuracy: 61.00%\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.4847 Accuracy: 62.48%\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.5320 Accuracy: 60.86%\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.4744 Accuracy: 60.86%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.5237 Accuracy: 61.00%\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.5578 Accuracy: 61.16%\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.4629 Accuracy: 62.74%\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.5271 Accuracy: 61.26%\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.4600 Accuracy: 62.06%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.5489 Accuracy: 60.62%\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.5953 Accuracy: 59.68%\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.4859 Accuracy: 61.72%\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.5323 Accuracy: 61.16%\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.4810 Accuracy: 61.46%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.5272 Accuracy: 61.30%\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.5398 Accuracy: 60.96%\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.4805 Accuracy: 62.04%\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.5621 Accuracy: 60.30%\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.5198 Accuracy: 60.94%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6099011480808259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

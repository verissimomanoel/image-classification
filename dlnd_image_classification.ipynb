{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = './cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ca0ea9be0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return ( x - np.min(x) ) / ( np.max(x) - np.min(x) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "x = tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # Weight and bias\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], x_tensor.get_shape().as_list()[3], \n",
    "                                              conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.random_normal([conv_num_outputs], stddev=0.1))\n",
    "    \n",
    "    # Applying convolution\n",
    "    x_tensor = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    # Add bias\n",
    "    x_tensor = tf.nn.bias_add(x_tensor, bias)\n",
    "\n",
    "    # Apply function of activation\n",
    "    x_tensor = tf.nn.relu(x_tensor)\n",
    "    \n",
    "    # Apply Max Pooling\n",
    "    x_tensor = tf.nn.max_pool(x_tensor, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                              strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "\n",
    "    return x_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (8, 8)\n",
    "    conv_strides = (4, 4)\n",
    "    pool_ksize = (4, 4)\n",
    "    pool_strides = (2, 2)\n",
    "    num_outputs = 10\n",
    "    \n",
    "    # 1, 2, or 3 Convolution and Max Pool layers\n",
    "    conv2d_maxpool_layer_1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, \n",
    "                                            pool_ksize, pool_strides)\n",
    "\n",
    "    conv2d_maxpool_layer_2 = conv2d_maxpool(conv2d_maxpool_layer_1, conv_num_outputs, conv_ksize, conv_strides, \n",
    "                                            pool_ksize, pool_strides)\n",
    "\n",
    "    conv2d_maxpool_layer_3 = conv2d_maxpool(conv2d_maxpool_layer_2, conv_num_outputs, conv_ksize, conv_strides,\n",
    "                                            pool_ksize, pool_strides)\n",
    "\n",
    "    # Flatten Layer\n",
    "    flatten_layer = flatten(conv2d_maxpool_layer_3)\n",
    "    #flatten_layer = tf.nn.dropout(flatten_layer, keep_prob)\n",
    "\n",
    "\n",
    "    # 1, 2, or 3 Fully Connected Layers\n",
    "    #fully_conn_layer = fully_conn(flatten_layer, num_outputs * 2)\n",
    "    #fully_conn_layer = tf.nn.dropout(fully_conn_layer, keep_prob)\n",
    "\n",
    "    fully_conn_layer_1 = fully_conn(flatten_layer, num_outputs * 50)\n",
    "    fully_conn_layer_1 = tf.nn.dropout(fully_conn_layer_1, keep_prob)\n",
    "    fully_conn_layer_2 = fully_conn(fully_conn_layer_1, num_outputs * 10)\n",
    "    fully_conn_layer_2 = tf.nn.dropout(fully_conn_layer_2, keep_prob)\n",
    "    fully_conn_layer_3 = fully_conn(fully_conn_layer_2, num_outputs * 5)\n",
    "    fully_conn_layer_3 = tf.nn.dropout(fully_conn_layer_3, keep_prob)\n",
    "\n",
    "    # Output Layer\n",
    "    #output_layer = output(fully_conn_layer, num_outputs)\n",
    "    output_layer = output(fully_conn_layer_3, num_outputs)\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1. })\n",
    "    accur = session.run(accuracy, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "    print('Loss: {:.4f} Accuracy: {:.2f}%'.format(loss, accur * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 150\n",
    "batch_size = 1024\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.3028 Accuracy: 10.15%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.3025 Accuracy: 10.02%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.2997 Accuracy: 13.12%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.2930 Accuracy: 17.45%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.2580 Accuracy: 16.46%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 2.2058 Accuracy: 19.31%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 2.1720 Accuracy: 19.80%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 2.1838 Accuracy: 22.28%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 2.0754 Accuracy: 24.50%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.9758 Accuracy: 25.99%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.9310 Accuracy: 28.47%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.9114 Accuracy: 28.47%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.8471 Accuracy: 29.95%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.8113 Accuracy: 32.92%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.7766 Accuracy: 34.65%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.7604 Accuracy: 33.29%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.7466 Accuracy: 33.66%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.7051 Accuracy: 37.00%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.6733 Accuracy: 38.00%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.6518 Accuracy: 37.87%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.6459 Accuracy: 38.86%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.6073 Accuracy: 39.11%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.5948 Accuracy: 39.98%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.5584 Accuracy: 41.71%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.5653 Accuracy: 41.71%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.6024 Accuracy: 38.37%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.5417 Accuracy: 39.73%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.5087 Accuracy: 43.44%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.5081 Accuracy: 44.43%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.4629 Accuracy: 44.80%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.5122 Accuracy: 43.19%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.4353 Accuracy: 45.67%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.4467 Accuracy: 45.54%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.4481 Accuracy: 45.67%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.4226 Accuracy: 48.51%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.4115 Accuracy: 46.91%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.3842 Accuracy: 48.51%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.4473 Accuracy: 46.53%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.3964 Accuracy: 47.77%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.4189 Accuracy: 49.38%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.3610 Accuracy: 49.26%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.3673 Accuracy: 48.51%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.3615 Accuracy: 49.88%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.3526 Accuracy: 50.25%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.3060 Accuracy: 51.49%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.3022 Accuracy: 53.09%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.3095 Accuracy: 50.99%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.3002 Accuracy: 53.59%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.2652 Accuracy: 53.09%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.2454 Accuracy: 55.45%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.2729 Accuracy: 53.34%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.2676 Accuracy: 54.46%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.2559 Accuracy: 53.47%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.2305 Accuracy: 57.30%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.2127 Accuracy: 54.70%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.2587 Accuracy: 53.71%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.2444 Accuracy: 55.32%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.1853 Accuracy: 57.18%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.1889 Accuracy: 56.81%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.1552 Accuracy: 58.54%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.1480 Accuracy: 59.03%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.1384 Accuracy: 58.54%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.1364 Accuracy: 59.03%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.1480 Accuracy: 57.92%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.1266 Accuracy: 59.65%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.1148 Accuracy: 59.78%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.0874 Accuracy: 62.38%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.0860 Accuracy: 60.52%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.0952 Accuracy: 61.01%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.0933 Accuracy: 60.77%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.0915 Accuracy: 61.39%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.1072 Accuracy: 61.01%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.1233 Accuracy: 61.51%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.1205 Accuracy: 58.91%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.0510 Accuracy: 62.75%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.0364 Accuracy: 63.00%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.0245 Accuracy: 63.49%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.0285 Accuracy: 63.24%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.0805 Accuracy: 59.90%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.0481 Accuracy: 62.75%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.0199 Accuracy: 63.86%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.0389 Accuracy: 63.37%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.0248 Accuracy: 63.61%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.0142 Accuracy: 63.37%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.0061 Accuracy: 63.86%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.0011 Accuracy: 62.87%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.0225 Accuracy: 62.62%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.9728 Accuracy: 65.59%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.0046 Accuracy: 63.12%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.9996 Accuracy: 62.38%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.9890 Accuracy: 64.73%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.9523 Accuracy: 65.72%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.9715 Accuracy: 64.73%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.9611 Accuracy: 64.98%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.9702 Accuracy: 64.60%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.9479 Accuracy: 65.59%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.9572 Accuracy: 65.22%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.9254 Accuracy: 65.97%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.9774 Accuracy: 65.59%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.0062 Accuracy: 64.23%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2894 Accuracy: 13.24%\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.2272 Accuracy: 18.07%\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 2.1320 Accuracy: 19.55%\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.0291 Accuracy: 27.10%\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.9839 Accuracy: 30.69%\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.9241 Accuracy: 32.67%\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.8358 Accuracy: 35.02%\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.7622 Accuracy: 37.87%\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.6814 Accuracy: 40.72%\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.6590 Accuracy: 39.98%\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.6153 Accuracy: 42.70%\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.6136 Accuracy: 43.19%\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.5475 Accuracy: 44.80%\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.5110 Accuracy: 45.17%\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.5187 Accuracy: 45.92%\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.5272 Accuracy: 45.79%\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.5254 Accuracy: 42.95%\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.4428 Accuracy: 48.27%\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.4290 Accuracy: 46.53%\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.4363 Accuracy: 50.25%\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.4231 Accuracy: 48.27%\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.4396 Accuracy: 46.16%\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.3608 Accuracy: 50.62%\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.3759 Accuracy: 49.13%\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.3789 Accuracy: 50.62%\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.3531 Accuracy: 50.74%\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.3741 Accuracy: 48.76%\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.2861 Accuracy: 54.46%\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.3041 Accuracy: 52.72%\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.3107 Accuracy: 54.33%\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.3659 Accuracy: 51.11%\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.3556 Accuracy: 50.87%\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.2518 Accuracy: 56.93%\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.2544 Accuracy: 55.45%\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.2627 Accuracy: 57.43%\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.2751 Accuracy: 55.07%\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.2792 Accuracy: 53.71%\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.1944 Accuracy: 56.56%\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.2334 Accuracy: 55.20%\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.2388 Accuracy: 58.54%\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.2642 Accuracy: 54.95%\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.2831 Accuracy: 54.21%\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.1837 Accuracy: 56.19%\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.2237 Accuracy: 56.19%\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.2109 Accuracy: 59.28%\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.2232 Accuracy: 57.05%\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.2350 Accuracy: 54.58%\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.1230 Accuracy: 59.78%\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.1611 Accuracy: 57.92%\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.1677 Accuracy: 61.88%\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.1939 Accuracy: 58.04%\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.1985 Accuracy: 55.94%\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.1136 Accuracy: 59.53%\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.1507 Accuracy: 57.92%\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.1450 Accuracy: 61.63%\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.1607 Accuracy: 60.15%\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.1538 Accuracy: 58.66%\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.0808 Accuracy: 62.25%\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.1160 Accuracy: 59.28%\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.1135 Accuracy: 61.14%\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.1569 Accuracy: 60.52%\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.1641 Accuracy: 58.17%\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.0780 Accuracy: 60.89%\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.1001 Accuracy: 61.26%\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.1053 Accuracy: 63.12%\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.1901 Accuracy: 57.18%\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.1483 Accuracy: 58.54%\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 1.0548 Accuracy: 62.13%\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.0996 Accuracy: 60.02%\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.0995 Accuracy: 64.60%\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.1895 Accuracy: 57.18%\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.1300 Accuracy: 59.28%\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.0949 Accuracy: 63.12%\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.0824 Accuracy: 61.51%\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.0893 Accuracy: 62.62%\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.1112 Accuracy: 62.00%\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.1126 Accuracy: 61.01%\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.0638 Accuracy: 63.24%\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.0482 Accuracy: 62.25%\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.0622 Accuracy: 64.85%\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.1270 Accuracy: 59.78%\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.1077 Accuracy: 60.89%\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 1.0569 Accuracy: 64.60%\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.0415 Accuracy: 63.49%\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.0360 Accuracy: 64.85%\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.1194 Accuracy: 60.02%\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.0990 Accuracy: 60.89%\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.0337 Accuracy: 63.24%\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.0164 Accuracy: 63.24%\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 1.0176 Accuracy: 65.22%\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.0587 Accuracy: 62.75%\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.0827 Accuracy: 61.63%\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 1.0138 Accuracy: 64.36%\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.0260 Accuracy: 63.00%\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.9980 Accuracy: 66.34%\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.0490 Accuracy: 62.87%\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.0725 Accuracy: 61.51%\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.9871 Accuracy: 65.22%\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.0044 Accuracy: 64.36%\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.9815 Accuracy: 67.45%\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.0552 Accuracy: 61.14%\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.0401 Accuracy: 62.38%\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.9718 Accuracy: 65.35%\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.9962 Accuracy: 65.35%\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.9617 Accuracy: 66.96%\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.0083 Accuracy: 65.10%\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.0130 Accuracy: 64.23%\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.9467 Accuracy: 66.46%\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.9803 Accuracy: 64.85%\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.9427 Accuracy: 68.69%\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.9813 Accuracy: 66.21%\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.9943 Accuracy: 64.23%\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.9394 Accuracy: 67.45%\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.9694 Accuracy: 65.97%\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.9243 Accuracy: 67.95%\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.9489 Accuracy: 66.46%\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 1.0020 Accuracy: 65.59%\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.9025 Accuracy: 68.94%\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.9480 Accuracy: 65.84%\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.9114 Accuracy: 68.44%\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.9675 Accuracy: 66.09%\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.9782 Accuracy: 65.84%\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.9135 Accuracy: 68.56%\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.9397 Accuracy: 65.72%\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.9045 Accuracy: 69.18%\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.9667 Accuracy: 64.60%\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.9516 Accuracy: 65.35%\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.9039 Accuracy: 68.94%\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.9592 Accuracy: 64.73%\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.9357 Accuracy: 67.95%\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.9320 Accuracy: 68.81%\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.9383 Accuracy: 66.83%\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.8906 Accuracy: 68.32%\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.9206 Accuracy: 66.96%\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.8748 Accuracy: 70.92%\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.9031 Accuracy: 66.96%\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.9122 Accuracy: 68.07%\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.8987 Accuracy: 68.19%\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.9066 Accuracy: 67.70%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.8521 Accuracy: 70.92%\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.8942 Accuracy: 69.31%\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.8902 Accuracy: 69.80%\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.8877 Accuracy: 67.45%\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.9029 Accuracy: 68.07%\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.8488 Accuracy: 71.04%\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.8820 Accuracy: 70.05%\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.8871 Accuracy: 69.31%\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.8765 Accuracy: 68.07%\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.9167 Accuracy: 67.45%\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.8600 Accuracy: 70.05%\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.8915 Accuracy: 69.31%\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.8792 Accuracy: 69.43%\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.8279 Accuracy: 70.79%\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.8825 Accuracy: 69.31%\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.8199 Accuracy: 71.04%\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.8648 Accuracy: 70.17%\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.8609 Accuracy: 69.43%\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.8555 Accuracy: 68.94%\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.8872 Accuracy: 68.81%\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.8315 Accuracy: 71.29%\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.8573 Accuracy: 70.17%\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.8516 Accuracy: 70.05%\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.8393 Accuracy: 71.16%\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.8736 Accuracy: 68.81%\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.8355 Accuracy: 71.16%\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.8539 Accuracy: 71.04%\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.8599 Accuracy: 70.05%\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.7933 Accuracy: 71.16%\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.8574 Accuracy: 70.05%\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.8308 Accuracy: 71.66%\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.8858 Accuracy: 69.80%\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.8957 Accuracy: 67.45%\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.8195 Accuracy: 71.29%\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.9002 Accuracy: 67.70%\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.8125 Accuracy: 72.77%\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.8256 Accuracy: 71.29%\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.8601 Accuracy: 70.05%\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.7965 Accuracy: 71.91%\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.8285 Accuracy: 71.29%\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.7956 Accuracy: 71.91%\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.8343 Accuracy: 70.92%\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.9117 Accuracy: 68.69%\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.7714 Accuracy: 72.15%\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.8431 Accuracy: 70.67%\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.7978 Accuracy: 73.14%\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.8037 Accuracy: 72.65%\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.8597 Accuracy: 69.93%\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.7821 Accuracy: 72.52%\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.8119 Accuracy: 70.92%\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.7562 Accuracy: 73.89%\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.8024 Accuracy: 71.66%\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.8431 Accuracy: 70.42%\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.7657 Accuracy: 72.40%\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.8084 Accuracy: 70.92%\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.7807 Accuracy: 72.65%\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.8420 Accuracy: 71.41%\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.8092 Accuracy: 70.79%\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.7810 Accuracy: 72.40%\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.8236 Accuracy: 70.17%\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.7913 Accuracy: 74.13%\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.8068 Accuracy: 70.42%\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.8108 Accuracy: 71.16%\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.7364 Accuracy: 73.64%\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.8133 Accuracy: 70.54%\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.7976 Accuracy: 73.14%\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.8169 Accuracy: 70.92%\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.8023 Accuracy: 71.29%\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.7606 Accuracy: 73.64%\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.8591 Accuracy: 69.18%\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.7868 Accuracy: 72.65%\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.7943 Accuracy: 71.66%\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.7957 Accuracy: 71.53%\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.7403 Accuracy: 74.13%\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.8199 Accuracy: 70.42%\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.7431 Accuracy: 74.38%\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.7664 Accuracy: 73.64%\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.7674 Accuracy: 73.64%\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.7152 Accuracy: 75.74%\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.7954 Accuracy: 71.04%\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.7701 Accuracy: 73.27%\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.7808 Accuracy: 73.51%\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.7841 Accuracy: 71.66%\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.7255 Accuracy: 74.88%\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.8001 Accuracy: 71.66%\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.7224 Accuracy: 76.36%\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.7895 Accuracy: 73.14%\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.7774 Accuracy: 71.91%\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.6958 Accuracy: 75.62%\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.8042 Accuracy: 71.41%\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.7171 Accuracy: 75.62%\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.7784 Accuracy: 73.39%\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.8186 Accuracy: 70.79%\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.7147 Accuracy: 75.12%\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.7601 Accuracy: 72.90%\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.7129 Accuracy: 75.74%\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.7617 Accuracy: 73.27%\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.7840 Accuracy: 71.66%\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.7099 Accuracy: 75.12%\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.7603 Accuracy: 72.65%\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.7361 Accuracy: 75.62%\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.7412 Accuracy: 74.01%\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.7826 Accuracy: 72.40%\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.7014 Accuracy: 74.13%\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.8257 Accuracy: 70.05%\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.7359 Accuracy: 74.50%\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.7513 Accuracy: 74.01%\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.7741 Accuracy: 73.51%\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.7151 Accuracy: 74.26%\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.7612 Accuracy: 73.14%\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.7092 Accuracy: 76.61%\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.7392 Accuracy: 74.75%\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.7867 Accuracy: 71.78%\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.6731 Accuracy: 76.61%\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.7445 Accuracy: 73.39%\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.7002 Accuracy: 76.49%\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.7258 Accuracy: 75.87%\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.7727 Accuracy: 71.91%\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.7159 Accuracy: 74.13%\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.7649 Accuracy: 70.17%\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.6847 Accuracy: 77.48%\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.7002 Accuracy: 75.12%\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.7376 Accuracy: 74.75%\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.6729 Accuracy: 75.25%\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.7247 Accuracy: 74.13%\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.6616 Accuracy: 77.72%\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.6912 Accuracy: 77.23%\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.7390 Accuracy: 74.26%\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.6550 Accuracy: 77.60%\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.7294 Accuracy: 72.77%\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.6539 Accuracy: 78.47%\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.6816 Accuracy: 76.86%\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.7386 Accuracy: 73.39%\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.6749 Accuracy: 75.87%\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.7033 Accuracy: 74.13%\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.6589 Accuracy: 76.61%\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.6779 Accuracy: 77.10%\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.7145 Accuracy: 75.37%\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.6432 Accuracy: 77.72%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.6905 Accuracy: 74.88%\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.6467 Accuracy: 78.09%\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.6621 Accuracy: 78.09%\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.6834 Accuracy: 76.11%\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.6278 Accuracy: 77.85%\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.6753 Accuracy: 76.86%\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.6380 Accuracy: 78.34%\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.6606 Accuracy: 78.09%\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.6833 Accuracy: 75.50%\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.6395 Accuracy: 76.73%\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.6711 Accuracy: 76.36%\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.6281 Accuracy: 77.72%\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.6547 Accuracy: 77.10%\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.6994 Accuracy: 75.00%\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.6349 Accuracy: 76.24%\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.6759 Accuracy: 75.50%\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.6403 Accuracy: 77.48%\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.6505 Accuracy: 78.84%\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.6495 Accuracy: 77.10%\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.6128 Accuracy: 78.59%\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.6820 Accuracy: 75.87%\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.6605 Accuracy: 77.85%\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.6467 Accuracy: 78.59%\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.6750 Accuracy: 76.86%\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.6191 Accuracy: 78.22%\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.7225 Accuracy: 75.37%\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.6709 Accuracy: 77.10%\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.6394 Accuracy: 78.22%\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.6755 Accuracy: 76.61%\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.6572 Accuracy: 76.61%\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.7343 Accuracy: 73.89%\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.6810 Accuracy: 77.10%\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.6502 Accuracy: 76.73%\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.7027 Accuracy: 75.62%\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.6761 Accuracy: 75.99%\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.7326 Accuracy: 74.13%\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.6527 Accuracy: 77.35%\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.6519 Accuracy: 79.33%\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.6863 Accuracy: 76.61%\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.6205 Accuracy: 78.34%\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.6808 Accuracy: 75.25%\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.6195 Accuracy: 79.70%\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.6248 Accuracy: 78.22%\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.6854 Accuracy: 76.36%\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.6273 Accuracy: 77.85%\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.7225 Accuracy: 74.13%\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.6481 Accuracy: 77.60%\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.6257 Accuracy: 78.84%\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.7214 Accuracy: 73.64%\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.6324 Accuracy: 77.85%\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.7227 Accuracy: 74.38%\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.6576 Accuracy: 77.10%\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.6766 Accuracy: 76.49%\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.6762 Accuracy: 76.11%\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.6270 Accuracy: 77.23%\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.6777 Accuracy: 76.61%\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.6465 Accuracy: 78.09%\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.6610 Accuracy: 77.85%\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.6621 Accuracy: 76.24%\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.5780 Accuracy: 81.06%\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.6504 Accuracy: 76.36%\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.6266 Accuracy: 78.71%\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.6240 Accuracy: 78.71%\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.6666 Accuracy: 75.50%\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.5863 Accuracy: 79.58%\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.6522 Accuracy: 77.23%\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.6351 Accuracy: 78.34%\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.6656 Accuracy: 76.36%\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.6649 Accuracy: 76.24%\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.5950 Accuracy: 78.47%\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.6485 Accuracy: 77.48%\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.6463 Accuracy: 77.72%\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.5985 Accuracy: 79.46%\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.6212 Accuracy: 77.85%\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.5719 Accuracy: 81.44%\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.6446 Accuracy: 77.10%\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.6486 Accuracy: 78.34%\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.5902 Accuracy: 79.70%\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.6622 Accuracy: 76.61%\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.5537 Accuracy: 81.31%\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.6446 Accuracy: 78.47%\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.6298 Accuracy: 78.84%\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.5995 Accuracy: 79.83%\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.6725 Accuracy: 76.98%\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.5951 Accuracy: 78.59%\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.6538 Accuracy: 77.10%\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.6101 Accuracy: 79.08%\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.6167 Accuracy: 77.85%\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.6040 Accuracy: 79.33%\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.6131 Accuracy: 78.84%\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.6373 Accuracy: 76.98%\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.6317 Accuracy: 78.59%\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.6025 Accuracy: 79.21%\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.6390 Accuracy: 77.48%\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.5966 Accuracy: 78.09%\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.6342 Accuracy: 77.48%\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.6092 Accuracy: 78.84%\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.5912 Accuracy: 79.46%\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.6121 Accuracy: 78.34%\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.5820 Accuracy: 78.22%\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.6326 Accuracy: 77.72%\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.5861 Accuracy: 80.57%\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.5787 Accuracy: 80.45%\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.6253 Accuracy: 78.09%\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.5780 Accuracy: 79.46%\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.6573 Accuracy: 76.24%\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.5986 Accuracy: 80.07%\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.5882 Accuracy: 79.83%\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.6161 Accuracy: 78.47%\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.5643 Accuracy: 78.84%\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.6135 Accuracy: 76.98%\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.5678 Accuracy: 80.82%\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.5871 Accuracy: 80.57%\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.6084 Accuracy: 78.47%\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.5355 Accuracy: 82.18%\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.6151 Accuracy: 78.22%\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.5892 Accuracy: 79.58%\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.5994 Accuracy: 78.84%\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.6106 Accuracy: 77.10%\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.5234 Accuracy: 81.56%\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.6119 Accuracy: 76.73%\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.5858 Accuracy: 79.58%\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.5502 Accuracy: 81.06%\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.5954 Accuracy: 78.09%\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.5072 Accuracy: 82.05%\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.5978 Accuracy: 78.34%\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.5737 Accuracy: 80.20%\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.5495 Accuracy: 80.57%\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.5790 Accuracy: 78.47%\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.5234 Accuracy: 82.05%\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.5702 Accuracy: 78.84%\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.5526 Accuracy: 80.57%\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.5369 Accuracy: 81.93%\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.5771 Accuracy: 78.09%\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.5201 Accuracy: 81.44%\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.5556 Accuracy: 80.82%\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.5518 Accuracy: 79.95%\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.5282 Accuracy: 83.04%\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.5626 Accuracy: 79.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.4994 Accuracy: 81.44%\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.5504 Accuracy: 81.68%\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.5617 Accuracy: 79.95%\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.5572 Accuracy: 80.69%\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.5461 Accuracy: 79.83%\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.5166 Accuracy: 81.93%\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.5680 Accuracy: 79.95%\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.5545 Accuracy: 81.31%\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.5751 Accuracy: 81.19%\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.5627 Accuracy: 80.07%\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.5143 Accuracy: 80.57%\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.5452 Accuracy: 81.06%\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.5178 Accuracy: 82.92%\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.5251 Accuracy: 82.80%\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.5550 Accuracy: 81.31%\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.5152 Accuracy: 81.68%\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.5406 Accuracy: 82.55%\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.5419 Accuracy: 81.81%\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.5124 Accuracy: 83.17%\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.5441 Accuracy: 80.45%\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.5221 Accuracy: 80.32%\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.5402 Accuracy: 81.31%\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.5464 Accuracy: 80.57%\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.5099 Accuracy: 83.42%\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.5630 Accuracy: 79.33%\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.5192 Accuracy: 82.18%\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.5645 Accuracy: 81.06%\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.5334 Accuracy: 81.93%\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.5469 Accuracy: 80.57%\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.6116 Accuracy: 78.96%\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.5309 Accuracy: 81.19%\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.5807 Accuracy: 80.82%\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.5400 Accuracy: 81.31%\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.5982 Accuracy: 78.71%\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.5757 Accuracy: 79.33%\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.5258 Accuracy: 81.31%\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.5489 Accuracy: 81.56%\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.5098 Accuracy: 83.42%\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.5319 Accuracy: 80.94%\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.5420 Accuracy: 80.45%\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.5011 Accuracy: 82.43%\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.5582 Accuracy: 81.44%\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.5213 Accuracy: 82.43%\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.5296 Accuracy: 82.05%\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.5371 Accuracy: 81.19%\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.5147 Accuracy: 82.05%\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.5352 Accuracy: 81.93%\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.5465 Accuracy: 80.69%\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.4990 Accuracy: 83.29%\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.5295 Accuracy: 81.31%\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.5539 Accuracy: 80.69%\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.5348 Accuracy: 81.19%\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.4959 Accuracy: 83.54%\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.4900 Accuracy: 84.65%\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.4960 Accuracy: 81.81%\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.4889 Accuracy: 81.93%\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.5124 Accuracy: 81.81%\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.4895 Accuracy: 83.79%\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.4708 Accuracy: 83.79%\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.4894 Accuracy: 82.18%\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.4653 Accuracy: 84.16%\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.4960 Accuracy: 81.93%\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.4730 Accuracy: 83.91%\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.4601 Accuracy: 85.02%\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.4954 Accuracy: 82.30%\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.4690 Accuracy: 82.67%\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.4986 Accuracy: 82.67%\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.4959 Accuracy: 82.05%\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.4620 Accuracy: 84.53%\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.4963 Accuracy: 82.43%\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.4652 Accuracy: 83.54%\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.4971 Accuracy: 82.30%\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.4734 Accuracy: 84.65%\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.4711 Accuracy: 83.17%\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.5123 Accuracy: 82.05%\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.4850 Accuracy: 81.44%\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.5170 Accuracy: 81.44%\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.4704 Accuracy: 84.41%\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.4958 Accuracy: 82.80%\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.5092 Accuracy: 82.18%\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.4580 Accuracy: 83.79%\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.5118 Accuracy: 80.94%\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.4692 Accuracy: 83.79%\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.5020 Accuracy: 83.54%\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 0.4901 Accuracy: 83.04%\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 0.4877 Accuracy: 82.92%\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 0.5022 Accuracy: 81.56%\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 0.4645 Accuracy: 84.28%\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.4581 Accuracy: 84.41%\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 0.5250 Accuracy: 80.45%\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 0.4711 Accuracy: 83.29%\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 0.5298 Accuracy: 81.31%\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 0.4590 Accuracy: 84.78%\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.4495 Accuracy: 84.78%\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 0.4868 Accuracy: 82.18%\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 0.4385 Accuracy: 86.01%\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 0.5165 Accuracy: 82.05%\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 0.5013 Accuracy: 83.42%\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.5270 Accuracy: 83.66%\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 0.4995 Accuracy: 81.44%\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 0.5227 Accuracy: 81.19%\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 0.5472 Accuracy: 80.69%\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 0.5880 Accuracy: 79.33%\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.5304 Accuracy: 82.67%\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 0.5313 Accuracy: 81.31%\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 0.5413 Accuracy: 81.81%\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 0.5625 Accuracy: 80.94%\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 0.6088 Accuracy: 77.72%\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.5196 Accuracy: 81.68%\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 0.5619 Accuracy: 79.95%\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 0.4672 Accuracy: 85.02%\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 0.5621 Accuracy: 79.08%\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 0.5082 Accuracy: 82.92%\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.5271 Accuracy: 80.82%\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 0.5275 Accuracy: 80.20%\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 0.4891 Accuracy: 83.54%\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 0.5386 Accuracy: 82.30%\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 0.5338 Accuracy: 82.43%\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.5457 Accuracy: 80.69%\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 0.5017 Accuracy: 81.44%\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 0.4721 Accuracy: 82.30%\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 0.5387 Accuracy: 81.19%\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 0.5304 Accuracy: 81.93%\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.5074 Accuracy: 82.43%\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 0.4831 Accuracy: 82.18%\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 0.4558 Accuracy: 83.91%\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 0.4998 Accuracy: 83.17%\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 0.5364 Accuracy: 82.43%\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.5225 Accuracy: 81.81%\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 0.5170 Accuracy: 81.81%\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 0.4616 Accuracy: 82.92%\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 0.4865 Accuracy: 83.04%\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 0.5207 Accuracy: 82.55%\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.5147 Accuracy: 82.67%\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 0.4956 Accuracy: 82.30%\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 0.4992 Accuracy: 82.05%\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 0.4943 Accuracy: 83.04%\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 0.5235 Accuracy: 82.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.5223 Accuracy: 81.68%\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 0.4941 Accuracy: 82.30%\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 0.4456 Accuracy: 83.66%\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 0.4726 Accuracy: 83.17%\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 0.5600 Accuracy: 80.20%\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.5131 Accuracy: 82.30%\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 0.4844 Accuracy: 83.17%\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 0.4755 Accuracy: 83.29%\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 0.4661 Accuracy: 84.03%\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 0.5398 Accuracy: 81.31%\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.5104 Accuracy: 82.30%\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 0.5130 Accuracy: 80.45%\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 0.4519 Accuracy: 85.15%\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 0.5541 Accuracy: 80.69%\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 0.5862 Accuracy: 79.83%\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.4773 Accuracy: 83.66%\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 0.5219 Accuracy: 79.70%\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 0.4277 Accuracy: 85.52%\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 0.5387 Accuracy: 80.20%\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 0.5583 Accuracy: 80.57%\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.5497 Accuracy: 80.07%\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 0.5106 Accuracy: 80.82%\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 0.5391 Accuracy: 80.94%\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 0.5965 Accuracy: 76.49%\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 0.5097 Accuracy: 83.29%\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.4998 Accuracy: 82.18%\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 0.4833 Accuracy: 82.30%\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 0.4845 Accuracy: 84.41%\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 0.5835 Accuracy: 79.21%\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 0.5257 Accuracy: 81.81%\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.5452 Accuracy: 81.81%\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 0.5493 Accuracy: 79.83%\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 0.4833 Accuracy: 84.16%\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 0.5385 Accuracy: 80.07%\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 0.4860 Accuracy: 84.16%\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.4944 Accuracy: 84.16%\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 0.5162 Accuracy: 81.81%\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 0.4844 Accuracy: 83.04%\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 0.4955 Accuracy: 82.30%\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 0.4879 Accuracy: 84.03%\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.4607 Accuracy: 84.90%\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 0.5201 Accuracy: 82.55%\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 0.4808 Accuracy: 83.79%\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 0.5282 Accuracy: 79.58%\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 0.4808 Accuracy: 83.17%\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.4504 Accuracy: 85.77%\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 0.4968 Accuracy: 83.91%\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 0.4531 Accuracy: 85.52%\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 0.4979 Accuracy: 81.06%\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 0.4722 Accuracy: 82.80%\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.4277 Accuracy: 86.26%\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 0.4933 Accuracy: 82.67%\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 0.4507 Accuracy: 83.91%\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 0.5095 Accuracy: 81.31%\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 0.4657 Accuracy: 84.78%\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.4255 Accuracy: 84.90%\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 0.5058 Accuracy: 80.94%\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 0.4285 Accuracy: 85.52%\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 0.4888 Accuracy: 83.17%\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 0.4732 Accuracy: 84.16%\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.4693 Accuracy: 83.17%\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 0.4504 Accuracy: 84.16%\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 0.4280 Accuracy: 84.65%\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 0.4850 Accuracy: 82.30%\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 0.4338 Accuracy: 85.27%\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.4055 Accuracy: 86.26%\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 0.4334 Accuracy: 84.53%\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 0.4084 Accuracy: 87.00%\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 0.4702 Accuracy: 84.16%\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 0.4284 Accuracy: 84.90%\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.3950 Accuracy: 86.63%\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 0.4142 Accuracy: 86.39%\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 0.3764 Accuracy: 86.63%\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 0.4294 Accuracy: 85.52%\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 0.3998 Accuracy: 86.76%\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.3776 Accuracy: 86.88%\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 0.4009 Accuracy: 84.90%\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 0.3581 Accuracy: 87.00%\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 0.4184 Accuracy: 86.26%\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 0.3894 Accuracy: 87.13%\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.3656 Accuracy: 88.86%\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 0.3862 Accuracy: 86.63%\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 0.3432 Accuracy: 88.00%\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 0.4071 Accuracy: 86.39%\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 0.3814 Accuracy: 87.87%\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.3591 Accuracy: 88.99%\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 0.3839 Accuracy: 86.51%\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 0.3532 Accuracy: 87.50%\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 0.4053 Accuracy: 85.64%\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 0.3783 Accuracy: 88.00%\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.3509 Accuracy: 88.86%\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 0.3912 Accuracy: 85.27%\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 0.3572 Accuracy: 88.24%\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 0.4192 Accuracy: 85.64%\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 0.3807 Accuracy: 88.37%\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.3551 Accuracy: 87.62%\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 0.3766 Accuracy: 86.26%\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 0.3691 Accuracy: 86.76%\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 0.3970 Accuracy: 86.76%\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 0.3943 Accuracy: 86.51%\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.3805 Accuracy: 86.88%\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 0.3869 Accuracy: 86.14%\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 0.3443 Accuracy: 88.37%\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 0.3849 Accuracy: 87.00%\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 0.3835 Accuracy: 87.75%\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.3429 Accuracy: 88.86%\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 0.3715 Accuracy: 86.88%\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 0.3292 Accuracy: 89.73%\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 0.3905 Accuracy: 86.76%\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 0.3688 Accuracy: 88.37%\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.3478 Accuracy: 89.11%\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 0.3572 Accuracy: 87.75%\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 0.3342 Accuracy: 88.24%\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 0.3907 Accuracy: 85.89%\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 0.3787 Accuracy: 86.88%\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.3577 Accuracy: 87.50%\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 0.3575 Accuracy: 87.25%\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 0.3293 Accuracy: 89.60%\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 0.3807 Accuracy: 87.87%\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 0.3692 Accuracy: 88.37%\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.3550 Accuracy: 88.99%\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 0.3572 Accuracy: 87.87%\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 0.3417 Accuracy: 87.87%\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 0.3690 Accuracy: 87.62%\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 0.3916 Accuracy: 87.75%\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.3279 Accuracy: 89.11%\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 0.3668 Accuracy: 86.51%\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 0.3606 Accuracy: 87.62%\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 0.3686 Accuracy: 87.13%\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 0.3887 Accuracy: 87.87%\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.3359 Accuracy: 89.48%\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 0.3822 Accuracy: 86.51%\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 0.3633 Accuracy: 87.62%\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 0.3889 Accuracy: 87.25%\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 0.4106 Accuracy: 87.13%\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.3811 Accuracy: 87.00%\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 0.3776 Accuracy: 86.51%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 0.3560 Accuracy: 87.50%\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 0.3906 Accuracy: 87.62%\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 0.3807 Accuracy: 87.38%\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.3504 Accuracy: 87.62%\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 0.4016 Accuracy: 86.14%\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 0.3762 Accuracy: 86.26%\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 0.3783 Accuracy: 87.38%\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 0.3494 Accuracy: 89.11%\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.3414 Accuracy: 88.61%\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 0.3984 Accuracy: 87.62%\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 0.3383 Accuracy: 88.12%\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 0.4049 Accuracy: 86.01%\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 0.3619 Accuracy: 89.11%\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.3109 Accuracy: 89.98%\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 0.4166 Accuracy: 86.01%\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 0.3645 Accuracy: 87.75%\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 0.3892 Accuracy: 86.76%\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 0.3884 Accuracy: 87.38%\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.3283 Accuracy: 89.11%\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 0.3862 Accuracy: 86.76%\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 0.3397 Accuracy: 88.24%\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 0.3737 Accuracy: 86.51%\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 0.3429 Accuracy: 88.37%\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.3157 Accuracy: 89.73%\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 0.3682 Accuracy: 87.50%\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 0.3440 Accuracy: 87.50%\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 0.3546 Accuracy: 87.38%\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 0.3463 Accuracy: 89.36%\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.3399 Accuracy: 88.49%\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 0.3562 Accuracy: 88.24%\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 0.3567 Accuracy: 87.38%\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 0.3791 Accuracy: 86.51%\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 0.3514 Accuracy: 88.24%\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.3378 Accuracy: 88.99%\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 0.3806 Accuracy: 86.63%\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 0.3260 Accuracy: 88.74%\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 0.3658 Accuracy: 86.76%\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 0.3946 Accuracy: 86.26%\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.3420 Accuracy: 88.12%\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 0.4055 Accuracy: 84.65%\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 0.3867 Accuracy: 86.01%\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 0.4305 Accuracy: 85.40%\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 0.3804 Accuracy: 87.50%\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.3155 Accuracy: 90.10%\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 0.4331 Accuracy: 83.91%\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 0.3527 Accuracy: 87.62%\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 0.3564 Accuracy: 88.49%\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 0.3711 Accuracy: 87.38%\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.3170 Accuracy: 90.72%\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 0.3866 Accuracy: 86.51%\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 0.3912 Accuracy: 87.25%\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 0.4142 Accuracy: 85.15%\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 0.3717 Accuracy: 87.38%\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.3541 Accuracy: 88.99%\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 0.3681 Accuracy: 86.39%\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 0.3562 Accuracy: 88.49%\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 0.4383 Accuracy: 84.41%\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 0.3869 Accuracy: 86.76%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6176717936992645\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiESaHIuIiIiIRP8f1lGgcALKkpcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c88591ac8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
